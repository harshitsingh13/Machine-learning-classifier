# -*- coding: utf-8 -*-
"""midsem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oxm5N5RBgxtOfw_6MZ4awwJHw8V_nSDR
"""



##drive.mount('/content/drive')
import pandas as pd
import numpy as np
from textblob import TextBlob
#import matplotlib.pyplot as plt1 
import matplotlib.pyplot as plt 
import math 
  
reddit_df = pd.read_pickle("/content/drive/My Drive/redditDataset.pkl")
data=reddit_df
data=data.drop_duplicates(subset=None, keep='first', inplace=False)
data=data.to_numpy()
n1=0
n2=0
p1=0
s1=0
p2=0
s2=0
poln=[]
polh=[]
subn=[]
subh=[]
count1=[]
count2=[]
count3=[]
count4=[]
spoln=[]
spolh=[]
ssubn=[]
ssubh=[]
pol=[]
sub=[]
for i in data:
  test=TextBlob(i[0])
  pol.append(test.sentiment.polarity)
  sub.append(test.sentiment.subjectivity)
  #print(i[1])
  if(i[1]=="news"):
    #print('no')
    n1+=1
    p1+=test.sentiment.polarity
    s1+=test.sentiment.subjectivity
    poln.append(test.sentiment.polarity)
    subn.append(test.sentiment.subjectivity)

  else:
    #print('yes')
    n2+=1
    p2+=test.sentiment.polarity
    s2+=test.sentiment.subjectivity
    polh.append(test.sentiment.polarity)
    subh.append(test.sentiment.subjectivity)
poln.sort()
subn.sort()
polh.sort()
subh.sort()
for i in range(0,len(poln)):
    if poln[i] not in spoln:
        spoln.append(poln[i])
    if subn[i] not in ssubn:
        ssubn.append(subn[i])
for i in range(0,len(polh)):
    if polh[i] not in spolh:
        spolh.append(polh[i])
    if subh[i] not in ssubh:
        ssubh.append(subh[i])



###########sum
#p1=sum(spoln)
#p2=sum(spolh)
#s1=sum(ssubn)
#s2=sum(ssubh)
#fig = plt.figure()
#plt1 = fig.add_subplot(221)
#plt2 = fig.add_subplot(222)
#plt3 = fig.add_subplot(223)
#plt4 = fig.add_subplot(224)

#print(n1,p1,s1,n2,p2,s2)
mean_p1=p1/n1
mean_p2=p2/n2
mean_s1=s1/n1
mean_s2=s2/n2
vp1=0
vp2=0
vs1=0
vs2=0
for i in range(0,len(spoln)):
  #vp1+=(spoln[i]-mean_p1)**2
  count1.append(poln.count(spoln[i]))
for i in range(0,len(spolh)):
  #vp2+=(spolh[i]-mean_p2)**2
  count2.append(polh.count(spolh[i]))
for i in range(0,len(ssubn)):
  #vs1+=(ssubn[i]-mean_s1)**2
  count3.append(subn.count(ssubn[i]))
for i in range(0,len(ssubh)):
  #vs2+=(ssubh[i]-mean_s2)**2
  count4.append(subh.count(ssubh[i]))

for i in data:
  test=TextBlob(i[0])
  if(i[1]=='news'):
    vp1+=(test.sentiment.polarity-mean_p1)**2
    vs1+=(test.sentiment.subjectivity-mean_s1)**2
  else:
    #print(vp2)
    vp2+=(test.sentiment.polarity-mean_p2)**2
    vs2+=(test.sentiment.subjectivity-mean_s2)**2

plt.plot(spoln,count1)
plt.xlabel('polarity of news')
# Set the y axis label of the current axis.
plt.ylabel('# of comments')
# Set a title 
#plt.title('Draw a line.')
plt.show()
#print(vs2)
var_p1=math.sqrt(vp1/n1)
var_p2=math.sqrt(vp2/n2)
var_s1=math.sqrt(vs1/n1)
var_s2=math.sqrt(vs2/n2)

plt.plot(spolh,count2)
plt.xlabel('polarity of humor')
# Set the y axis label of the current axis.
plt.ylabel('# of comments')
plt.show()

plt.plot(ssubn,count3)
plt.xlabel('subjectivity of news')
# Set the y axis label of the current axis.
plt.ylabel('# of comments')
plt.show()

plt.plot(ssubh,count4)
plt.xlabel('subjectivity of humor')
# Set the y axis label of the current axis.
plt.ylabel('# of comments')
plt.show()

print('Mean of news polarity is:',mean_p1)
print('Mean of humor polarity is:',mean_p2)
print('Mean of news subjectivity is:',mean_s1)
print('Mean of humor subjectivity is:',mean_s2)
print('Standard deviation of news polarity:',var_p1)
print('Standard deviation of humor polarity:',var_p2)
print('Standard deviation of news subjectivity:',var_s1)
print('Standard deviation of humor subjectivity:',var_s2)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from textblob import TextBlob
from nltk import word_tokenize 
from nltk.util import ngrams
from nltk.corpus import stopwords 
from collections import Counter
import matplotlib.pyplot as plt 
import pandas as pd

from heapq import nlargest

reddit_df = pd.read_pickle("/content/drive/My Drive/redditDataset.pkl")
data=reddit_df
data=data.drop_duplicates(subset=None, keep='first', inplace=False)
data=data.to_numpy()
new=data.tolist()
#str=data.tostring()
stop_words = set(stopwords.words('english'))
stop_words.add('nt')
dictun={}
dictuh={}
dictbn={}
dictbh={}
l1=[]
l2=[]
count=[]
#for collecting unigrams of news and humour
for i in data:
  if(i[1]=="news"):
    l1.extend(i[0].split())
  else:
    l2.extend(i[0].split())
new=[]
bigramn=[]
bigramh=[]
unigram=[]
#for collecting bigrams of news and humour
for i in data:
    #print(i[0])
    if(i[1]=="news"):
        token = nltk.word_tokenize(i[0])
        bigramn.extend(list(ngrams(token, 2)))
    if(i[1]=="humor"):
        token = nltk.word_tokenize(i[0])
        bigramh.extend(list(ngrams(token, 2)))
    #unigram=list(ngrams(token,1))


#for counting unigrams comes under news
for w in l1: 
  if w not in stop_words:
        if w in dictun: 
            dictun[w] = dictun[w] + 1
        else: 
            dictun[w] = 1
#for counting unigrams comes under humour
for w in l2: 
  if w not in stop_words:
        if w in dictuh: 
            dictuh[w] = dictuh[w] + 1
        else: 
            dictuh[w] = 1
#for counting bigrams of news
for w in bigramn: 
  if w[0] not in stop_words and w[1] not in stop_words:
        #print(w)
        if w in dictbn: 
            dictbn[w] = dictbn[w] + 1
        else: 
            dictbn[w] = 1
#for counting bigrams of humour
for w in bigramh: 
  if w[0] not in stop_words and w[1] not in stop_words:
        if w in dictbh: 
            dictbh[w] = dictbh[w] + 1
        else: 
            dictbh[w] = 1


#for plotting distribution
def get_value(key1,dict):
    for key, value in dict.items():
         if key1 == key:
             return value


resultun = nlargest(10, dictun, key = dictun.get)
resultuh = nlargest(10, dictuh, key = dictuh.get)
resultbn = nlargest(10, dictbn, key = dictbn.get)
resultbh = nlargest(10, dictbh, key = dictbh.get)
freq1=[]
freq2=[]
freq3=[]
freq4=[]
for i in resultun:
  freq1.append(get_value(i,dictun))
for i in resultuh:
  freq2.append(get_value(i,dictuh))
for i in resultbn:
  freq3.append(get_value(i,dictbn))
for i in resultbh:
  freq4.append(get_value(i,dictbh))
#print(freq1)
#print(freq2)
#print(freq3)
#print(freq4)
print("top 10 unigrams of news:",resultun)
print("top 10 unigrams of humor:",resultuh)
print("top 10 bigrams of news:",resultbn)
print("top 10 bigrams of humor:",resultbh)
#print(type(resultun))
#print(type(resultbn))
#print(get_value('nt',dictun))
#print(unigram)

plt.bar(resultun,freq1)
plt.xlabel('unigrams of news')
plt.ylabel('frequency of unigrams')
print('Here I take "nt" under stopwords')
plt.title("top 10 unigrams of news without stopwords")
plt.xticks(rotation=90)
plt.show()

plt.bar(resultuh,freq2)
plt.xlabel('unigrams of humor')
plt.ylabel('frequency of unigrams')
plt.title("top 10 unigrams of humor without stopwords")
plt.xticks(rotation=90)
plt.show()

for i in range(len(resultbn)):
  resultbn[i]=str(resultbn[i])
plt.bar(resultbn,freq3)
plt.xlabel("bigrams of news")
plt.ylabel("frequency of bigrams")
plt.title("top 10 bigrams of news without stopwords")
plt.xticks(rotation=90)
plt.show()

for i in range(len(resultbh)):
  resultbh[i]=str(resultbh[i])
plt.bar(resultbh,freq4)
plt.xlabel("bigrams of humor")
plt.ylabel("frequency of bigrams")
plt.title("top 10 bigrams of humor without stopwords")
plt.xticks(rotation=90)
plt.show()

import nltk
nltk.download('punkt')
from sklearn.linear_model import LogisticRegression as lm
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score
import csv
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
import pandas as pd
import numpy as np
from sklearn import svm
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer
#import matplotlib.pyplot as plt1 
from sklearn.model_selection import train_test_split
from nltk.tokenize import word_tokenize
from collections import defaultdict 
 
reddit_df = pd.read_pickle("/content/drive/My Drive/redditDataset.pkl")
data=reddit_df
data=data.drop_duplicates(subset=None, keep='first', inplace=False)
data=data.to_numpy()
newdata=data
single=[]
l=[]
for i in newdata:
  l.append(i[0])
  single.extend(i[0].split())
  #print(i[0])
  if(i[1]=="news"):
    i[1]=1
  else:
    i[1]=0
#print(l)
sentences = []
idx_word = {}
allwords = []
vector=[]

for i in l:
    newsent = word_tokenize(i)
    #print(newsent)
    for w in newsent:
      #print(i)
      if w.isalpha():
        w.lower()
    sentences.append(newsent)
    for j in newsent:
        allwords.append(j)
#print(allwords)
comments=[]
for i in data:
  comments.append(i[0])
#number of words in the vocabulary
length = len(allwords)
idx=len(sentences)
j = 0
for i in allwords:
    idx_word[i] = j 
    j += 1
res=[]
for i in newdata:
  res.append(i[1])
for i in range(0,idx):
  #print(i)
  count = defaultdict(int)
  vec = np.zeros(length)
  #vec=vec.tolist()
  for j in sentences[i]:
    count[j] += 1
  for k,m in count.items():
    vec[idx_word[k]] = m
  vector.append(vec) 

#print(len(sentences))
#print(len(res))
#print(len(vector))
regenerate=list(zip(vector,res))
#print(regenerate)
X_train, X_test, y_train, y_test = train_test_split(l, res, test_size=0.30, random_state=0)
#train=list(zip(X_train,y_train))
#test=list(zip(X_test,y_test))
###################################
print("Here I use logistic regression model for finding results")
#################################################
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)
#################################################
#print(len(train))
#print(len(test))
lm1=lm(random_state=0)
lm1.fit(X_train,y_train)
predictions=lm1.predict(X_test)
print("Accuracy score is:",accuracy_score(y_test, predictions))
print("f1 score is:",f1_score(y_test,predictions))
print("Precision score is:",precision_score(y_test,predictions))
print("Recall score is:",recall_score(y_test,predictions))
print("Confusion matrix is:")
print(confusion_matrix(y_test,predictions))
#print(len(predictions),len(y_test),len(y_train))
#print(len(res))

well=[]
fail=[]
c1=0
c2=0
for i in range(0,len(predictions)):
  if((predictions[i]==res[i]) and c1<5):
    well.append(i)
    c1+=1
  if((predictions[i]!=res[i]) and c2<5):
    fail.append(i)
    c2+=1
#print(well,fail)
#print(len(predictions))
c1=1
c2=1
#print('section 1')
for i in well:
  print("Comment",c1,"where model performed well:")
  print(comments[i])
  c1+=1
for i in fail:
  print("Comment",c2,"where model is fail:")
  print(comments[i])
  c2+=1
##  print

#############section-3#####################
import csv
import matplotlib.pyplot as plt
import nltk
import numpy as np
import seaborn as sns
#from jaccard import distance
rows=[]
data=[]
twt=[]
face=[]
inst=[]
name=[]

#Algorithm is taken from https://stackoverflow.com/questions/46975929/how-can-i-calculate-the-jaccard-similarity-of-two-lists-containing-strings-in-py
#Jaccard similarity between two string

def jaccard_similarity(list1, list2):
    s1 = set(list1)
    s2 = set(list2)
    return float(len(s1.intersection(s2)) / len(s1.union(s2)))

############Algorithm ends here##############

#algorithm is taken from https://www.nltk.org/_modules/nltk/metrics/distance.html
#Jaro similarity between two strings

def jaro_similarity(s1, s2):
    # First, store the length of the strings
    # because they will be re-used several times.
    len_s1, len_s2 = len(s1), len(s2)

    # The upper bound of the distance for being a matched character.
    match_bound = max(len_s1, len_s2) // 2 - 1

    # Initialize the counts for matches and transpositions.
    matches = 0  # no.of matched characters in s1 and s2
    transpositions = 0  # no. of transpositions between s1 and s2
    flagged_1 = []  # positions in s1 which are matches to some character in s2
    flagged_2 = []  # positions in s2 which are matches to some character in s1

    # Iterate through sequences, check for matches and compute transpositions.
    for i in range(len_s1):  # Iterate through each character.
        upperbound = min(i + match_bound, len_s2 - 1)
        lowerbound = max(0, i - match_bound)
        for j in range(lowerbound, upperbound + 1):
            if s1[i] == s2[j] and j not in flagged_2:
                matches += 1
                flagged_1.append(i)
                flagged_2.append(j)
                break
    flagged_2.sort()
    for i, j in zip(flagged_1, flagged_2):
        if s1[i] != s2[j]:
            transpositions += 1

    if matches == 0:
        return 0
    else:
        return (
            1
            / 3
            * (
                matches / len_s1
                + matches / len_s2
                + (matches - transpositions // 2) / matches
            )
        )
###########################################
############Algorithm ends here##############

#Algorithm is taken from https://www.nltk.org/_modules/nltk/metrics/distance.html
#Jaro Winkler Similarity between two strings

def jaro_winkler_similarity(s1, s2, p=0.1, max_l=4):
    """
    The Jaro Winkler distance is an extension of the Jaro similarity in:

        William E. Winkler. 1990. String Comparator Metrics and Enhanced
        Decision Rules in the Fellegi-Sunter Model of Record Linkage.
        Proceedings of the Section on Survey Research Methods.
        American Statistical Association: 354-359.
    such that:

        jaro_winkler_sim = jaro_sim + ( l * p * (1 - jaro_sim) )

    where,

        - jaro_sim is the output from the Jaro Similarity,
        see jaro_similarity()
        - l is the length of common prefix at the start of the string
            - this implementation provides an upperbound for the l value
              to keep the prefixes.A common value of this upperbound is 4.
        - p is the constant scaling factor to overweigh common prefixes.
          The Jaro-Winkler similarity will fall within the [0, 1] bound,
          given that max(p)<=0.25 , default is p=0.1 in Winkler (1990)


    Test using outputs from https://www.census.gov/srd/papers/pdf/rr93-8.pdf
    from "Table 5 Comparison of String Comparators Rescaled between 0 and 1"

    >>> winkler_examples = [("billy", "billy"), ("billy", "bill"), ("billy", "blily"),
    ... ("massie", "massey"), ("yvette", "yevett"), ("billy", "bolly"), ("dwayne", "duane"),
    ... ("dixon", "dickson"), ("billy", "susan")]

    >>> winkler_scores = [1.000, 0.967, 0.947, 0.944, 0.911, 0.893, 0.858, 0.853, 0.000]
    >>> jaro_scores =    [1.000, 0.933, 0.933, 0.889, 0.889, 0.867, 0.822, 0.790, 0.000]

        # One way to match the values on the Winkler's paper is to provide a different
    # p scaling factor for different pairs of strings, e.g.
    >>> p_factors = [0.1, 0.125, 0.20, 0.125, 0.20, 0.20, 0.20, 0.15, 0.1]

    >>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
    ...     assert round(jaro_similarity(s1, s2), 3) == jscore
    ...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore


    Test using outputs from https://www.census.gov/srd/papers/pdf/rr94-5.pdf from
    "Table 2.1. Comparison of String Comparators Using Last Names, First Names, and Street Names"

    >>> winkler_examples = [('SHACKLEFORD', 'SHACKELFORD'), ('DUNNINGHAM', 'CUNNIGHAM'),
    ... ('NICHLESON', 'NICHULSON'), ('JONES', 'JOHNSON'), ('MASSEY', 'MASSIE'),
    ... ('ABROMS', 'ABRAMS'), ('HARDIN', 'MARTINEZ'), ('ITMAN', 'SMITH'),
    ... ('JERALDINE', 'GERALDINE'), ('MARHTA', 'MARTHA'), ('MICHELLE', 'MICHAEL'),
    ... ('JULIES', 'JULIUS'), ('TANYA', 'TONYA'), ('DWAYNE', 'DUANE'), ('SEAN', 'SUSAN'),
    ... ('JON', 'JOHN'), ('JON', 'JAN'), ('BROOKHAVEN', 'BRROKHAVEN'),
    ... ('BROOK HALLOW', 'BROOK HLLW'), ('DECATUR', 'DECATIR'), ('FITZRUREITER', 'FITZENREITER'),
    ... ('HIGBEE', 'HIGHEE'), ('HIGBEE', 'HIGVEE'), ('LACURA', 'LOCURA'), ('IOWA', 'IONA'), ('1ST', 'IST')]

    >>> jaro_scores =   [0.970, 0.896, 0.926, 0.790, 0.889, 0.889, 0.722, 0.467, 0.926,
    ... 0.944, 0.869, 0.889, 0.867, 0.822, 0.783, 0.917, 0.000, 0.933, 0.944, 0.905,
    ... 0.856, 0.889, 0.889, 0.889, 0.833, 0.000]

    >>> winkler_scores = [0.982, 0.896, 0.956, 0.832, 0.944, 0.922, 0.722, 0.467, 0.926,
    ... 0.961, 0.921, 0.933, 0.880, 0.858, 0.805, 0.933, 0.000, 0.947, 0.967, 0.943,
    ... 0.913, 0.922, 0.922, 0.900, 0.867, 0.000]

        # One way to match the values on the Winkler's paper is to provide a different
    # p scaling factor for different pairs of strings, e.g.
    >>> p_factors = [0.1, 0.1, 0.1, 0.1, 0.125, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.20,
    ... 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]


    >>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
    ...     if (s1, s2) in [('JON', 'JAN'), ('1ST', 'IST')]:
    ...         continue  # Skip bad examples from the paper.
    ...     assert round(jaro_similarity(s1, s2), 3) == jscore
    ...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore



    This test-case proves that the output of Jaro-Winkler similarity depends on
    the product  l * p and not on the product max_l * p. Here the product max_l * p > 1
    however the product l * p <= 1

    >>> round(jaro_winkler_similarity('TANYA', 'TONYA', p=0.1, max_l=100), 3)
    0.88


    """
    # To ensure that the output of the Jaro-Winkler's similarity
    # falls between [0,1], the product of l * p needs to be
    # also fall between [0,1].
    if not 0 <= max_l * p <= 1:
        warnings.warn(
            str(
                "The product  `max_l * p` might not fall between [0,1]."
                "Jaro-Winkler similarity might not be between 0 and 1."
            )
        )

    # Compute the Jaro similarity
    jaro_sim = jaro_similarity(s1, s2)

    # Initialize the upper bound for the no. of prefixes.
    # if user did not pre-define the upperbound,
    # use shorter length between s1 and s2

    # Compute the prefix matches.
    l = 0
    # zip() will automatically loop until the end of shorter string.
    for s1_i, s2_i in zip(s1, s2):
        if s1_i == s2_i:
            l += 1
        else:
            break
        if l == max_l:
            break
    # Return the similarity value as described in docstring.
    return jaro_sim + (l * p * (1 - jaro_sim))

############Algorithm ends here##############

##################################################################

with open('/content/drive/My Drive/IdentityResolution.csv', 'r') as file:
    reader = csv.reader(file)
    
    for row in reader:
      data.append(row)
      #print(row[3])
      name.append(row[0])
      #print(row[3].rsplit('/',1)[1])
      #print(row[1])
      #print(row[1].rsplit('/',1)[1])
      twt.append(row[1].rsplit('com/',1)[1])
      #print(row[2])
      #print(row[2].rsplit('/',1)[1])
      face.append(row[2].rsplit('com/',1)[1])
      #print(row[3])
      #print(row[3].rsplit('com/',1)[1])
      inst.append(row[3].rsplit('com/',1)[1])
      rows.append(row)
disttf=[]
distfi=[]
distti=[]
simtf=[]
simfi=[]
simti=[]
jsimtf=[]
jsimfi=[]
jsimti=[]
jasimtf=[]
jasimfi=[]
jasimti=[]
for i in range(0,len(twt)):
    dist1=nltk.jaccard_distance(set(twt[i]), set(face[i]))
    dist2=nltk.jaccard_distance(set(face[i]), set(inst[i]))
    dist3=nltk.jaccard_distance(set(twt[i]), set(inst[i]))
    disttf.append(dist1)
    distfi.append(dist2)
    distti.append(dist3)

    #jaccard similarity
    simtf.append(jaccard_similarity(twt[i],face[i]))
    simfi.append(jaccard_similarity(face[i],inst[i]))
    simti.append(jaccard_similarity(twt[i],inst[i]))

    #jaro similarity
    jsimtf.append(jaro_similarity(twt[i],face[i]))
    jsimfi.append(jaro_similarity(face[i],inst[i]))
    jsimti.append(jaro_similarity(twt[i],inst[i]))

    #Jaro winkler similarity
    jasimtf.append(jaro_winkler_similarity(twt[i],face[i]))
    jasimfi.append(jaro_winkler_similarity(face[i],inst[i]))
    jasimti.append(jaro_winkler_similarity(twt[i],inst[i]))

print('usernames of twitter, facebook, instagram respectively')
print(twt)
print(face)
print(inst)
#here I use three distance metrics in which jaro similarity is best to use
print(disttf)
print('\ntwitter-facebook similarity by jaccard distance metrics')
print(simtf)
print('\ntwitter-facebook similarity by jaro distance metrics')
print(jsimtf)
print('\ntwitter-facebook similarity by jaro winkler distance metrics')
print(jasimtf)
print('\nfacebook-instagram similarity by jaccard distance metrics')
print(simfi)
print('\nfacebook-instagram similarity by jaro distance metrics')
print(jsimfi)
print('\nfacebook-instagram similarity by jaro winkler metrics')
print(jasimfi)
print('\ntwitter-instagram similarity by jaccard distance metrics')
print(simti)
print('\ntwitter-instagram similarity by jaro distance metrics')
print(jsimti)
print('\ntwitter-instagram similarity by jaro winkler distance metrics')
print(jasimti)
###############################
print('\nmean of jaccard, jaro, jaro winkler distance metrics respectively for twitter-facebook similarity')
print(sum(simtf)/len(simtf),sum(jsimtf)/len(jsimtf),sum(jasimtf)/len(jasimtf))
print('\nmean of jaccard, jaro, jaro winkler distance metrics respectively for facebook-instagram similarity')
print(sum(simfi)/len(simfi),sum(jsimfi)/len(jsimfi),sum(jasimfi)/len(jasimfi))
print('\nmean of jaccard, jaro, jaro winkler distance metrics respectively for twitter-instagram similarity')
print(sum(simti)/len(simti),sum(jsimti)/len(jsimti),sum(jasimti)/len(jasimti))

#print(mean)
n1=[]
n2=[]
n3=[]
n4=[]
n5=[]
n6=[]
n7=[]
n8=[]
n9=[]
#print(len(simtf),len(jsimtf),len(jasimtf),len(simfi),len(jsimfi),len(jasimfi),len(simti),len(jsimti),len(jasimti))
#distribution plotting
for i in range(0,len(twt)):
    if simtf[i] not in n1:
        n1.append(simtf[i])
    if jsimtf[i] not in n2:
        n2.append(jsimtf[i])
    if jasimtf[i] not in n3:
        n3.append(jasimtf[i])
    if simfi[i] not in n4:
        n4.append(simfi[i])
    if jsimfi[i] not in n5:
        n5.append(jsimfi[i])
    if jasimfi[i] not in n6:
        n6.append(jasimfi[i])
    if simti[i] not in n7:
        n7.append(simti[i])
    if jsimti[i] not in n8:
        n8.append(jsimti[i])
    if jasimti[i] not in n9:
        n9.append(jasimti[i])
#print(len(n1),len(n2),len(n3),len(n4),len(n5),len(n6),len(n7),len(n8),len(n9))
#for counting frequencies
n1.sort()
n2.sort()
n3.sort()
n4.sort()
n5.sort()
n6.sort()
n7.sort()
n8.sort()
n9.sort()
c1=[]
c2=[]
c3=[]
c4=[]
c5=[]
c6=[]
c7=[]
c8=[]
c9=[]

for i in range(0,len(n1)):
  c1.append(simtf.count(n1[i]))
for i in range(0,len(n2)):
  c2.append(jsimtf.count(n2[i]))
for i in range(0,len(n3)):
  c3.append(jasimtf.count(n3[i]))
for i in range(0,len(n4)):
  c4.append(simfi.count(n4[i]))
for i in range(0,len(n5)):
  c5.append(jsimfi.count(n5[i]))
for i in range(0,len(n6)):
  c6.append(jasimfi.count(n6[i]))
for i in range(0,len(n7)):
  c7.append(simti.count(n7[i]))
for i in range(0,len(n8)):
  c8.append(jsimti.count(n8[i]))
for i in range(0,len(n9)):
  c9.append(jasimti.count(n9[i]))


print('\nHere I use three distance metrics: jaccard distance metrics, jaro distance metrics, jaro winkler distance metrics')
print('\nOut of these three distance metrics the mean of jaro winkler distance metrics is high among all three conditions, so it is best to use')
#sns.distplot(simtf, label='twitter-facebook', hist=False)
plt.plot(n1,c1)
plt.title('twitter-facebook similarity by jaccard distance metrics')
plt.xlabel('similarity distribution')
plt.ylabel('no. of users')
plt.show()
plt.plot(n2,c2)
plt.title('twitter-facebook similarity by jaro distance metrics')
plt.xlabel('similarity distribution')
plt.ylabel('no. of users')
plt.show()
plt.plot(n3,c3)
plt.title('twitter-facebook similarity by jaro winkler distance metrics')
plt.xlabel('similarity distribution')
plt.ylabel('no. of users')
plt.show()
plt.plot(n4,c4)
plt.title('facebook-instagram similarity by jaccard distance metrics')
plt.xlabel('similarity distribution')
plt.ylabel('no. of users')
plt.show()
plt.plot(n5,c5)
plt.title('facebook-instagram similarity by jaro distance metrics')
plt.xlabel('similarity distribution')
plt.ylabel('no. of users')
plt.show()
plt.plot(n6,c6)
plt.title('facebook-instagram similarity by jaro winkler distance metrics')
plt.xlabel('similarity distribution')
plt.ylabel('no. of users')
plt.show()
plt.plot(n7,c7)
plt.title('twitter-instagram similarity by jaccard distance metrics')
plt.xlabel('similarity distribution')
plt.ylabel('no. of users')
plt.show()
plt.plot(n8,c8)
plt.title('twitter-instagram similarity by jaro distance metrics')
plt.xlabel('similarity distribution')
plt.ylabel('no. of users')
plt.show()
plt.plot(n9,c9)
plt.xlabel('similarity distribution')
plt.title('twitter-instagram similarity by jaro winkler distance metrics')
plt.ylabel('no. of users')
plt.show()

print('section-4')

"""a) It is not possible to predict aadhar number in India because aadhar number is generated by using random numbers and not generated by using any statistical pattern, contact number, date of birth or any other personal data which is used to generate ssn number in US. Also, it is accesed by any other party which have some statistical knowledge by using data which is available publicly and hence detect statistical patterns in the SSN from it.

c) Somewhere it is good to use account holder's information by online social media platforms like facebook, twitter, instagram, etc because it's really useful for us in some situations where we need support by social media platforms, some of the reasons are given below:
1) Suppose if you forgot your login id or password or both then you can find your profile by taking help from services given by those social media platform where your profile is exist.
2) When someone is trying to hack your account, the social media platform use account holder's information against the third party which try to forcefully login to other's account, so here by using our information prevents missuse of our account.
3) By using our information by social media helps to see how users view, share and engage with their contents which helps in building a good social media network of peoples around us those have similar interest and point of view like us.
4) By using our some personal information social media helps us in finding peoples of our mutual ones which makes our communication to those peoples easier.
5) By using peoples information, social media can see their likes or dislikes by doing research on their activity and then try to make changes in their system or applications to make platform more better for users and also helps to figure out that what the users are expected or want from their social media platform which makes the attraction of users more attractive towards their platform.
"""

import csv
import matplotlib.pyplot as plt
import nltk
import numpy as np

file="/content/drive/My Drive/loc-brightkite_totalCheckins.txt"
f=open(file,"r")
lines=f.readlines()
result1=[]
result=[]
for x in lines:
     result1.append(x)
f.close()

lines_seen = set()  # holds lines already seen
outfile = open('new.txt', "w")
infile = open(file, "r")
#print "The file bar.txt is as follows"
for line in infile:
    #print line
    if((line not in lines_seen)):  # not a duplicate
        outfile.write(line)
        lines_seen.add(line)
outfile.close()
#print "The file foo.txt is as follows"
user=[]
new=[]
checkin=[]
latitude=[]
longitude=[]
locationid=[]
for line in open('new.txt', "r"):
    result.append(line)
    user.append(line[0])
    checkin.append(line[1])
    latitude.append(line[2])
    longitude.append(line[3])
    locationid.append(line[4])
v=0
for i in result:
  #print(i)
  if (i[2]== 0.0 and i[3]== 0.0):
    result.remove(i)
    #line.

    
#print(len(result))
#print(len(result))
#user.sort()
res=sorted(range(len(user)), key=lambda i: user[i])[-10:]
  #print(i)
#print(user[:30])
c=[]
j=0
loc=[]

for i in range(len(res)):
  c.append(user.count(res[i]))
#print(c)

